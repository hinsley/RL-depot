{"cells":[{"metadata":{},"cell_type":"markdown","source":"This is my first attempt at an OpenAI Gym environment. I am using vanilla REINFORCE to learn a policy for the CartPole environment."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!pip install jax jaxlib","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install gym","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\n\nrandom.seed(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We need 4 inputs and 2 softmax outputs for our policy network.\n\nimport jax.numpy as jnp\nimport jax.tree_util as tree_util\nfrom jax import grad, jit, vmap\nfrom jax import random as jrandom\n\nfrom jax.experimental import stax\nfrom jax.experimental.stax import Dense, Relu, Softmax\n\npolicy_init_fun, policy_net = stax.serial(Dense(256), Relu,\n                                          Dense(2), Softmax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Update rule for REINFORCE:\n\n$\\theta \\leftarrow \\theta + \\alpha G_t \\nabla \\ln \\pi(A_t \\mid S_t, \\theta)$"},{"metadata":{"trusted":true},"cell_type":"code","source":"def select_action(policy_params, state):\n    \"\"\"\n    Select an action according to a sample from the policy distribution.\n    \"\"\"\n    \n    # Select an action.\n    policy_dist = policy_net(policy_params, jnp.array(state))\n    action = random.choices([0, 1], weights=policy_dist)[0]\n    \n    return action","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\n# This was used for debugging.\ndef sum_params(params) -> float:\n    acc = 0.0\n    for param in params:\n        if type(param) != np.float32:\n            acc += sum_params(param)\n        else:\n            acc += param\n    return acc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"step_size = 1e-3\nconsecutive_solutions_required = 3 # How many times should we have to solve the task in a row to stop training?\n\nfrom jax.experimental.optimizers import adam\n\nopt_init, opt_update, opt_get_params = adam(step_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gym\n\n# Initialize the environment.\nenv = gym.make(\"CartPole-v1\")\nobservation = env.reset()\n\n# Initialize parameters.\npolicy_output_shape, policy_params = policy_init_fun(jrandom.PRNGKey(0), (1, 4))\n\n# Initialize optimizer.\nopt_state = opt_init(policy_params)\n\n# This accumulator speeds up the parameter update computation.\ntotal_reward = 0\n# This is a list of tuples containing (state, action, reward).\nepisode_SARs = []\n\n# Used for plotting.\ntotal_rewards = []\n\nconsecutive_solutions = 0\nepisode = 1\nprint(f\"Training until reward threshold {env.spec.reward_threshold} is attained.\")\nwhile True:\n    action = select_action(policy_params, observation)\n    previous_observation = observation\n    observation, reward, done, info = env.step(action)\n    \n    # Update episode training data.\n    total_reward += reward\n    episode_SARs.append((previous_observation, action, reward))\n    \n    if done:\n        total_rewards.append(total_reward)\n        print(f\"Episode {episode} reward achieved: {int(total_reward)}.\")\n        \n        if total_reward >= env.spec.reward_threshold:\n            consecutive_solutions += 1\n            if consecutive_solutions == consecutive_solutions_required:\n                # Training complete.\n                print(\"Task solved!\")\n                break\n        else:\n            consecutive_solutions = 0\n        \n        # Reset the state to initial conditions.\n        observation = env.reset()\n        \n        # Update the parameters.\n        def loss(policy_params, total_reward):\n            acc = 0\n            for _state, _action, _reward in episode_SARs:\n                # Increase the loss.\n                acc -= total_reward * jnp.log(policy_net(policy_params, _state)[_action])\n                # Update the return for the next step.\n                total_reward -= _reward\n            \n            return acc\n        \n        policy_grad = grad(loss)(policy_params, total_reward)\n        \n        opt_state = opt_update(episode, policy_grad, opt_state)\n        \n        # Grab the updated parameters.\n        policy_params = opt_get_params(opt_state)\n        \n        episode_SARs = []\n        episode += 1\n        total_reward = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib import pyplot as plt\n\nplt.plot(total_rewards)\nplt.xlabel(\"Episode\")\nplt.ylabel(\"Total reward\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!apt-get install python-opengl -y\n!pip install pyvirtualdisplay","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyvirtualdisplay import Display\nimport os\n\ndisplay = Display(visible=0, size=(1400, 900))\ndisplay.start()\nos.environ[\"DISPLAY\"] = \":\" + str(display.display) + \".\" + str(display._obj._screen)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib import animation, rc\n\nfig = plt.figure()\n\nframe = []\n\ntotal_reward = 0\nobservation = env.reset()\n\nwhile True:\n    action = select_action(policy_params, observation)\n    observation, reward, done, info = env.step(action)\n    \n    total_reward += reward\n    \n    img = plt.imshow(env.render(\"rgb_array\"))\n    frame.append([img])\n    if done:\n        break\n\nanim = animation.ArtistAnimation(fig, frame, interval=100, repeat_delay=1000, blit=True)\nrc(\"animation\", html=\"jshtml\")\n\nprint(f\"Final reward: {total_reward}.\")\n\nanim","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}