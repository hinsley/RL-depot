{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TDc4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMrIfoczpg3I//MuPNuZu6J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hinsley/RL-depot/blob/master/TDc4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyOpjEmlsnSz",
        "colab_type": "text"
      },
      "source": [
        "# TDc4\n",
        "\n",
        "Temporal difference learning model for Connect 4\n",
        "\n",
        "---\n",
        "\n",
        "Uses value function approximation inspired by [this paper](https://link.springer.com/content/pdf/10.1007/s10994-012-5280-0.pdf)\n",
        "\n",
        "State features: 1x1, 2x2, 3x3, 4x4 local configurations, reflected horizontally for weight sharing, with location dependent and location independent weight vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2-mTpv_5nHR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import requests\n",
        "from collections import Counter\n",
        "from random import choice, choices\n",
        "from time import time\n",
        "from typing import Dict, List, Optional, Tuple, Union"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHhoJAmntbFs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PositionState = int # See below.\n",
        "VALUE_BLANK = 0\n",
        "VALUE_X = 1\n",
        "VALUE_O = 2\n",
        "\n",
        "GameState = np.ndarray # 6 x 7 (Rows x Cols)\n",
        "\n",
        "LocalFeature = np.ndarray # [1, 4] x [1, 4] (Rows x Cols)\n",
        "\n",
        "EncodedFeature = int # Does not include any indication of local feature size.\n",
        "\n",
        "# Indexed by feature size, and row/column of upper-leftmost position. Row and column are -1 for location-independent features.\n",
        "EncodedFeatureVector = Dict[int, Dict[int, Dict[int, List[EncodedFeature]]]]\n",
        "\n",
        "# Indexed by feature size, and row/column of upper-leftmost position. Row and column are -1 for location-independent features.\n",
        "WeightVector = Dict[int, Dict[int, Dict[int, Dict[EncodedFeature, float]]]]\n",
        "\n",
        "StateValue = float # [0.0, 1.0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKIcyxv880pl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pprint(state: GameState):\n",
        "  \"\"\" Displays a given game configuration. \"\"\"\n",
        "  def rasterize_position(position_state: PositionState) -> str:\n",
        "    return {\n",
        "        VALUE_BLANK: \" \",\n",
        "        VALUE_X: \"X\",\n",
        "        VALUE_O: \"O\",\n",
        "    }[position_state]\n",
        "\n",
        "  print(\" __ _ _ _ _ _ __\")\n",
        "  for i, row in enumerate(state):\n",
        "    print(f\"\"\"{i+1}|{' '.join([rasterize_position(position_state) for\n",
        "                              position_state in\n",
        "                              row])}|\"\"\")\n",
        "  print(\" -- - - - - - --\")\n",
        "  print(\"  A B C D E F G \")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zI9SXd4B9zBQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def empty_game_state() -> GameState:\n",
        "  \"\"\" Generates an empty, new game state. \"\"\"\n",
        "  rows = 6\n",
        "  cols = 7\n",
        "  return np.full((rows, cols), VALUE_BLANK)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ER5qVy2D5-ED",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eval_state(state: GameState, check_game_over: bool=False) -> Union[StateValue, bool]:\n",
        "  \"\"\"\n",
        "  Takes as input a game state and returns a reward (for X winning), punishment\n",
        "  (for X losing), or a neutral state value (unexplored ambiguity). This is NOT\n",
        "  called for states that have already been explored. If check_game_over is\n",
        "  True, returns a boolean value that is True if the game is over, and False\n",
        "  otherwise.\n",
        "  \"\"\"\n",
        "  if np.all(state != VALUE_BLANK):\n",
        "    if check_game_over:\n",
        "      return True\n",
        "    else:\n",
        "      return 0.0 # Draw!\n",
        "\n",
        "  def check_horizontal_wins(state: GameState) -> StateValue:\n",
        "    for row in state:\n",
        "      for col in range(len(row) - 3):\n",
        "        if (row[col:col+4] == row[col]).all() and row[col] in [VALUE_X, VALUE_O]:\n",
        "          return float(row[col] == VALUE_X) * 2 - 1\n",
        "    return 0.0 # Ambiguous state.\n",
        "\n",
        "  horizontal_win = check_horizontal_wins(state)\n",
        "\n",
        "  if horizontal_win != 0.0:\n",
        "    if check_game_over:\n",
        "      return True\n",
        "    else:\n",
        "      return horizontal_win\n",
        "  \n",
        "  vertical_win = check_horizontal_wins(state.T)\n",
        "\n",
        "  if vertical_win != 0.0:\n",
        "    if check_game_over:\n",
        "      return True\n",
        "    else:\n",
        "      return vertical_win\n",
        "\n",
        "  def check_diagonal_wins(state: GameState) -> StateValue:\n",
        "    for row in range(len(state) - 3):\n",
        "      for col in range(len(state[row]) - 3):\n",
        "        if all([state[row, col] == state[row + offset, col + offset] for offset in range(1, 4)]) and state[row, col] in [VALUE_X, VALUE_O]:\n",
        "          return float(state[row, col] == VALUE_X) * 2 - 1\n",
        "    return 0.0 # Ambiguous state.\n",
        "  \n",
        "  backslash_diagonal_win = check_diagonal_wins(state)\n",
        "\n",
        "  if backslash_diagonal_win != 0.0:\n",
        "    if check_game_over:\n",
        "      return True\n",
        "    else:\n",
        "      return backslash_diagonal_win\n",
        "  \n",
        "  slash_diagonal_win = check_diagonal_wins(np.flip(state, axis=1))\n",
        "\n",
        "  if slash_diagonal_win != 0.0:\n",
        "    if check_game_over:\n",
        "      return True\n",
        "    else:\n",
        "      return slash_diagonal_win\n",
        "  \n",
        "  if check_game_over:\n",
        "    return False\n",
        "  else:\n",
        "    return 0.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfXanHGcDnkx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def active_features(state: GameState) -> List[Tuple[int, int, int, LocalFeature]]:\n",
        "  \"\"\"\n",
        "  Takes in a board state and returns the active features.\n",
        "\n",
        "  For each active feature, we return a tuple consisting of the local feature\n",
        "  size (width or height), the row index of the top left position within the\n",
        "  feature, the column index of the top left position within the feature, and\n",
        "  the feature itself.\n",
        "  \"\"\"\n",
        "  local_features = []\n",
        "\n",
        "  for local_size in range(1, 5): # local_size is width and height of a feature.\n",
        "    for row in range(state.shape[0] - local_size + 1):\n",
        "      for col in range(state.shape[1] - local_size + 1):\n",
        "        local_feature = state[row : row + local_size, col : col + local_size]\n",
        "\n",
        "        # Location-independent feature.\n",
        "        local_features.append((\n",
        "            local_size,\n",
        "            -1,\n",
        "            -1,\n",
        "            local_feature,\n",
        "        ))\n",
        "\n",
        "        # Location-dependent feature.\n",
        "        local_features.append((\n",
        "          local_size,\n",
        "          row,\n",
        "          col,\n",
        "          local_feature,\n",
        "        ))\n",
        "  \n",
        "  return local_features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXNK36hxVQf1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode_feature(feature: LocalFeature) -> EncodedFeature:\n",
        "  \"\"\" Encodes a local feature so that it is hashable for tabular indexing. \"\"\"\n",
        "  encoding = 0\n",
        "\n",
        "  for row in range(feature.shape[0]):\n",
        "    for col in range(feature.shape[1]):\n",
        "      encoding += feature[row, col] * 3 ** (row * feature.shape[0] + col)\n",
        "\n",
        "  return encoding"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rSnJA0WkjcG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def drop_piece(state: GameState, column: int, x_move: bool) -> GameState:\n",
        "  drop_row = state.shape[0] - 1 # In case entire column is empty.\n",
        "\n",
        "  for row in range(state.shape[0]):\n",
        "    if state[row][column] != VALUE_BLANK:\n",
        "      drop_row = row - 1\n",
        "      break\n",
        "\n",
        "  new_state = state.copy()\n",
        "  new_state[drop_row][column] = VALUE_X if x_move else VALUE_O\n",
        "  \n",
        "  return new_state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6E5Hy20CFtvR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode_game_state(state: GameState) -> EncodedFeatureVector:\n",
        "  encoding = {i: {-1: {-1: []}} for i in range(1, 5)}\n",
        "  \n",
        "  for feature in active_features(state):\n",
        "    encoded_feature = encode_feature(feature[3])\n",
        "    if not feature[0] in encoding: # Generate local feature size dictionary.\n",
        "      encoding[feature[0]] = dict()\n",
        "    if not feature[1] in encoding[feature[0]]: # Generate row dictionary.\n",
        "      encoding[feature[0]][feature[1]] = dict()\n",
        "    if not feature[2] in encoding[feature[0]][feature[1]]: # Generate column list.\n",
        "      encoding[feature[0]][feature[1]][feature[2]] = []\n",
        "    encoding[feature[0]][feature[1]][feature[2]].append(encoded_feature)\n",
        "  \n",
        "  return encoding"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqPWGXZHudo0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weight_vector: WeightVector = {i: {-1: {-1: dict()}} for i in range(1, 5)} # We can already create the row/column keys associated with the location-independent weight vector."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etZg1aL0XA3-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Agent():\n",
        "\n",
        "  x_player: bool # X player will try to maximize reward, O player will try to minimize it.\n",
        "  _prev_encoded_feature_vector: EncodedFeatureVector\n",
        "  _prev_state_value: float\n",
        "\n",
        "  def __init__(self, x_player: bool):\n",
        "    self.x_player = x_player\n",
        "    self.reset_game_state()\n",
        "  \n",
        "  def reset_game_state(self):\n",
        "    state = empty_game_state()\n",
        "\n",
        "    self._prev_encoded_feature_vector = encode_game_state(state)\n",
        "    self._prev_state_value = self.evaluate(state)\n",
        "\n",
        "    # TODO: Implement location dependence.\n",
        "  \n",
        "  def evaluate(self, state: GameState) -> float:\n",
        "    if eval_state(state, check_game_over=True):\n",
        "      return eval_state(state, check_game_over=False)\n",
        "\n",
        "    features: List[Tuple[int, int, int, LocalFeature]] = active_features(state)\n",
        "\n",
        "    accumulator: float = 0\n",
        "    \n",
        "    for feature in features:\n",
        "      # TODO: Implement mirroring / player reversal.\n",
        "      encoded_feature = encode_feature(feature[3])\n",
        "      if not feature[0] in weight_vector:\n",
        "        weight_vector[feature[0]] = dict()\n",
        "      if not feature[1] in weight_vector[feature[0]]:\n",
        "        weight_vector[feature[0]][feature[1]] = dict()\n",
        "      if not feature[2] in weight_vector[feature[0]][feature[1]]:\n",
        "        weight_vector[feature[0]][feature[1]][feature[2]] = dict()\n",
        "      if not encoded_feature in weight_vector[feature[0]][feature[1]][feature[2]]:\n",
        "        weight_vector[feature[0]][feature[1]][feature[2]][encoded_feature] = eval_state(state)\n",
        "\n",
        "      accumulator += weight_vector[feature[0]][feature[1]][feature[2]][encoded_feature]\n",
        "\n",
        "    return 1 / (1 + math.exp(-accumulator)) # Sigmoid to squash to [0.0, 1.0].\n",
        "\n",
        "  def td_update(self, new_state: GameState, learning_rate: float=0.03) -> float:\n",
        "    \"\"\"\n",
        "    This function could be sped up a lot by cacheing state value, among other\n",
        "    things.\n",
        "    \"\"\"\n",
        "    new_state_value = self.evaluate(new_state)\n",
        "\n",
        "    feature_count: Dict[int, Dict[int, Dict[int, Counter[EncodedFeature]]]] = {i: {-1: {-1: None}} for i in range(1, 5)}\n",
        "\n",
        "    signal_power = 0 # Used for normalization to make learning rate invariant under feature variation.\n",
        "    for local_size in self._prev_encoded_feature_vector:\n",
        "      for row in self._prev_encoded_feature_vector[local_size]:\n",
        "        for col in self._prev_encoded_feature_vector[local_size][row]:\n",
        "          if not row in feature_count[local_size]:\n",
        "            feature_count[local_size][row] = dict()\n",
        "          feature_count[local_size][row][col] = Counter(self._prev_encoded_feature_vector[local_size][row][col])\n",
        "          signal_power += sum([feature_count[local_size][row][col][encoded_feature] ** 2 for encoded_feature in feature_count[local_size][row][col]])\n",
        "    \n",
        "    for local_size in feature_count:\n",
        "      for row in feature_count[local_size]:\n",
        "        if not row in weight_vector[local_size]:\n",
        "          weight_vector[local_size][row] = dict()\n",
        "        for col in feature_count[local_size][row]:\n",
        "          if not col in weight_vector[local_size][row]:\n",
        "            weight_vector[local_size][row][col] = dict()\n",
        "          for encoded_feature in feature_count[local_size][row][col]:\n",
        "            if not encoded_feature in weight_vector[local_size][row][col]:\n",
        "              weight_vector[local_size][row][col][encoded_feature] = 0\n",
        "            weight_vector[local_size][row][col][encoded_feature] += learning_rate * feature_count[local_size][row][col][encoded_feature] / signal_power * (new_state_value - self._prev_state_value)\n",
        "\n",
        "    # Store values needed for next update.\n",
        "    self._prev_encoded_feature_vector = encode_game_state(new_state)\n",
        "    self._prev_state_value = new_state_value\n",
        "\n",
        "    return new_state_value\n",
        "  \n",
        "  def best_move(self, state: GameState, think_time: float=0.0, epsilon: float=0.15) -> GameState:\n",
        "    def rollout(initial_state: GameState, time_allowance: float):\n",
        "      initial_time = time()\n",
        "\n",
        "      x_player = Agent(True)\n",
        "      o_player = Agent(False)\n",
        "\n",
        "      rollout_state = initial_state\n",
        "\n",
        "      rollouts = 0\n",
        "      while time() - initial_time < time_allowance:\n",
        "        rollout_state = initial_state\n",
        "        while True:\n",
        "          rollout_state = x_player.best_move(rollout_state, think_time=0.0)\n",
        "          if eval_state(rollout_state, check_game_over=True):\n",
        "            rollouts += 1\n",
        "            break\n",
        "          rollout_state = o_player.best_move(rollout_state, think_time=0.0)\n",
        "          if eval_state(rollout_state, check_game_over=True):\n",
        "            rollouts += 1\n",
        "            break\n",
        "    \n",
        "    rollout(initial_state=state, time_allowance=think_time)\n",
        "\n",
        "    # Exploitation-Exploration selection.\n",
        "    if choices([True, False], weights=[epsilon, 1.0-epsilon])[0]:\n",
        "      # Explore.\n",
        "      action_column = choice([i for\n",
        "                              i in\n",
        "                              range(state.shape[1]) if\n",
        "                              state[0, i] == VALUE_BLANK])\n",
        "      new_state = drop_piece(state, action_column, self.x_player)\n",
        "      self.td_update(new_state)\n",
        "      return new_state\n",
        "      \n",
        "    # Exploit.\n",
        "    optimum_value = 0.0 if self.x_player else 1.0 # Initialize with the worst possible value.\n",
        "    optimum_states: List[GameState] = []\n",
        "    for column in range(state.shape[1]):\n",
        "      if state[0, column] == VALUE_BLANK: # Column is not full of pieces.\n",
        "        new_state = drop_piece(state, column, self.x_player)\n",
        "\n",
        "        state_value = self.td_update(new_state)\n",
        "        \n",
        "        if (self.x_player and state_value >= optimum_value) or (not self.x_player and state_value <= optimum_value):\n",
        "          if optimum_value != state_value: # New optimum state found.\n",
        "            optimum_value = state_value\n",
        "            optimum_states = []\n",
        "          optimum_states.append(new_state)\n",
        "    \n",
        "    new_state = choice(optimum_states)\n",
        "    self.td_update(new_state)\n",
        "\n",
        "    return new_state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tqfBqaqd8yH",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Reset knowledge\n",
        "\n",
        "weight_vector: WeightVector = {i: {-1: {-1: dict()}} for i in range(1, 5)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsu4zT2SFPIm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def optimal_move(state: GameState, x_move: bool) -> GameState:\n",
        "  scores = requests.get(f\"http://kevinalbs.com/connect4/back-end/index.php/getMoves?board_data={''.join(['0'] * 7 + [''.join([str(col) for col in row]) for row in state])}&player={'1' if x_move else '2'}\").json()\n",
        "  return int(max(scores, key=scores.get))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nI73L7Z7Gz-3",
        "colab_type": "code",
        "cellView": "form",
        "outputId": "c8d0026d-70c9-4fc8-9ead-9577df592b02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 655
        }
      },
      "source": [
        "#@title Training by Self-Play\n",
        "\n",
        "x_player = Agent(True)\n",
        "o_player = Agent(False)\n",
        "\n",
        "games =   500#@param {type: \"number\"}\n",
        "think_time = 0.1#@param {type: \"number\"}\n",
        "show_games = False #@param {type: \"boolean\"}\n",
        "\n",
        "start_time = time()\n",
        "for game in range(games):\n",
        "  game_state = empty_game_state()\n",
        "  if show_games:\n",
        "    pprint(game_state)\n",
        "  while True:\n",
        "    if show_games:\n",
        "      print()\n",
        "    game_state = x_player.best_move(game_state, think_time=think_time, epsilon=0.2)\n",
        "    # game_state = drop_piece(game_state, optimal_move(game_state, True), True)\n",
        "    if show_games:\n",
        "      pprint(game_state)\n",
        "    if eval_state(game_state, check_game_over=True):\n",
        "      print(f\"Game {game+1:,} of {games:,} ({(game+1)/games:.3%}): X wins!\")\n",
        "      break\n",
        "    if show_games:\n",
        "      print()\n",
        "    game_state = o_player.best_move(game_state, think_time=think_time, epsilon=0.2)\n",
        "    # game_state = drop_piece(game_state, optimal_move(game_state, False), False)\n",
        "    if show_games:\n",
        "      pprint(game_state)\n",
        "    if np.all(game_state != VALUE_BLANK):\n",
        "      print(f\"Game {game+1:,} of {games:,} ({(game+1)/games:.3%}): Draw!\")\n",
        "      break\n",
        "    if eval_state(game_state, check_game_over=True):\n",
        "      print(f\"Game {game+1:,} of {games:,} ({(game+1)/games:.3%}): O wins!\")\n",
        "      break\n",
        "\n",
        "time_elapsed = time() - start_time\n",
        "\n",
        "print(f\"Played {games:,} games in {time_elapsed:,.2f} seconds.\")"
      ],
      "execution_count": 246,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Game 1 of 500 (0.200%): X wins!\n",
            "Game 2 of 500 (0.400%): X wins!\n",
            "Game 3 of 500 (0.600%): O wins!\n",
            "Game 4 of 500 (0.800%): X wins!\n",
            "Game 5 of 500 (1.000%): O wins!\n",
            "Game 6 of 500 (1.200%): X wins!\n",
            "Game 7 of 500 (1.400%): O wins!\n",
            "Game 8 of 500 (1.600%): X wins!\n",
            "Game 9 of 500 (1.800%): X wins!\n",
            "Game 10 of 500 (2.000%): X wins!\n",
            "Game 11 of 500 (2.200%): X wins!\n",
            "Game 12 of 500 (2.400%): O wins!\n",
            "Game 13 of 500 (2.600%): O wins!\n",
            "Game 14 of 500 (2.800%): X wins!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-246-2d82739b80dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshow_games\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mgame_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_player\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_move\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthink_time\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthink_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;31m# game_state = drop_piece(game_state, optimal_move(game_state, True), True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshow_games\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-245-d70886c0bdab>\u001b[0m in \u001b[0;36mbest_move\u001b[0;34m(self, state, think_time, epsilon)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m     \u001b[0mrollout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_allowance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthink_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;31m# Exploitation-Exploration selection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-245-d70886c0bdab>\u001b[0m in \u001b[0;36mrollout\u001b[0;34m(initial_state, time_allowance)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mrollout_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m           \u001b[0mrollout_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_player\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_move\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrollout_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthink_time\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0meval_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrollout_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_game_over\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mrollouts\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-245-d70886c0bdab>\u001b[0m in \u001b[0;36mbest_move\u001b[0;34m(self, state, think_time, epsilon)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdrop_piece\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_player\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mstate_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtd_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_player\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mstate_value\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0moptimum_value\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_player\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mstate_value\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0moptimum_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-245-d70886c0bdab>\u001b[0m in \u001b[0;36mtd_update\u001b[0;34m(self, new_state, learning_rate)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mthings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \"\"\"\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mnew_state_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mfeature_count\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEncodedFeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-245-d70886c0bdab>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m       \u001b[0;31m# TODO: Implement mirroring / player reversal.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m       \u001b[0mencoded_feature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencode_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mweight_vector\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mweight_vector\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-f19d9ac9a46b>\u001b[0m in \u001b[0;36mencode_feature\u001b[0;34m(feature)\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m       \u001b[0mencoding\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrhysLPfWOS_",
        "colab_type": "code",
        "cellView": "form",
        "outputId": "a54c7c84-c096-4d37-e790-6970cbf132e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "source": [
        "#@title Play as O\n",
        "\n",
        "# Note: Control flow completely does not work for player to play as X yet. Need\n",
        "# to totally rethink this.\n",
        "\n",
        "column = \"B\" #@param [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\"]\n",
        "think_time =  10#@param {type: \"number\"}\n",
        "\n",
        "col_i = ord(column) - ord(\"A\")\n",
        "\n",
        "try:\n",
        "  turn = turn\n",
        "except:\n",
        "  turn = 1\n",
        "  playing_as_x = False\n",
        "\n",
        "print(f\"Turn {turn}\")\n",
        "\n",
        "if turn == 1:\n",
        "  game_state = empty_game_state()\n",
        "\n",
        "  if playing_as_x:\n",
        "    game_state = drop_piece(game_state, col_i, x_move=playing_as_x)\n",
        "  else:\n",
        "    game_state = x_player.best_move(game_state, think_time=think_time, epsilon=0.01)\n",
        "\n",
        "  print(f\"Win Probability: {x_player.evaluate(game_state):.2%}\")\n",
        "  pprint(game_state)\n",
        "\n",
        "  turn += 1\n",
        "else:\n",
        "  if playing_as_x:\n",
        "    game_state = o_player.best_move(game_state, think_time=think_time, epsilon=0.01)\n",
        "  else:\n",
        "    game_state = drop_piece(game_state, col_i, x_move=playing_as_x)\n",
        "\n",
        "  if eval_state(game_state, check_game_over=True):\n",
        "    pprint(game_state)\n",
        "    turn = 1\n",
        "    print(\"O wins!\")\n",
        "  else:\n",
        "    # Draw. This can only happen after O's turn.\n",
        "    if np.all(game_state != VALUE_BLANK):\n",
        "      turn = 1\n",
        "      print(\"Draw!\")\n",
        "    else:\n",
        "      if playing_as_x:\n",
        "        game_state = drop_piece(game_state, col_i, x_move=playing_as_x)\n",
        "      else:\n",
        "        game_state = x_player.best_move(game_state, think_time=think_time, epsilon=0.0)\n",
        "\n",
        "      print(f\"Win Probability: {x_player.evaluate(game_state):.2%}\")\n",
        "      pprint(game_state)\n",
        "\n",
        "      if eval_state(game_state, check_game_over=True):\n",
        "        turn = 1\n",
        "        print(\"X wins!\")\n",
        "      else:\n",
        "        turn += 1"
      ],
      "execution_count": 285,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Turn 17\n",
            "Win Probability: 41.79%\n",
            " __ _ _ _ _ _ __\n",
            "1|X O   O X X  |\n",
            "2|O X   X O X  |\n",
            "3|X O   X O X  |\n",
            "4|O O   O O O  |\n",
            "5|X O   O X X X|\n",
            "6|X X O O O X X|\n",
            " -- - - - - - --\n",
            "  A B C D E F G \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oMpCZJ6ztSM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weight_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2S6xfYv2r35H",
        "colab_type": "text"
      },
      "source": [
        "# Algorithm\n",
        "\n",
        "- [X] Start with a game state $s_t$.\n",
        "- [X] For the afterstate of each possible action:\n",
        "  - [X] Enumerate the local shape features.\n",
        "  - [X] Look up each local shape feature in the weights table.\n",
        "    - [X] Implement location dependence.\n",
        "  - [X] Sum all the weights associated with each feature up.\n",
        "  - [X] Apply the logistic sigmoid function to this linear sum. The result of this calculation is the associated action's afterstate value.\n",
        "    - $V(s) = \\sigma(\\phi(s) \\cdot \\theta^{LI} + \\phi(s) \\cdot \\theta^{LD})$\n",
        "- [X] Select the next action which results in the greatest afterstate value $V(s_{t+1})$.\n",
        "  - [X] Other player tries to minimize $V(s_{t+1})$ instead of maximize it.\n",
        "  - [X] $\\epsilon$-greedy selection\n",
        "- [X] Save what you need such that you can do the following update after your opponent makes a move and creates the new state $s_{t+2}$:\n",
        "  - [X] $\\Delta\\theta^{LD} = \\Delta\\theta^{LI} = \\alpha \\frac{\\phi(s_t)}{||\\phi(s_t)||^2}(V(s_{t+2})-V(s_t))$\n",
        "  - [X] Save $\\phi(s_t)$ and $V(s_t)$, as well as a reference to $\\theta_t$ so that it can be updated by $\\Delta\\theta$.\n"
      ]
    }
  ]
}